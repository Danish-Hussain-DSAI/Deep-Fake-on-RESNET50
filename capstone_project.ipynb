{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danish-Hussain-DSAI/Deep-Fake-on-RESNET50/blob/main/capstone_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyKSsLBYAKGK",
        "outputId": "8378b3b9-4b0b-4f09-c883-43be48a0012b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "pyKSsLBYAKGK"
    },
    {
      "cell_type": "code",
      "source": [
        "#!7za -y x \"/content/drive/MyDrive/ds fake and real images/real.zip\""
      ],
      "metadata": {
        "id": "HEg-_goLPXw7"
      },
      "id": "HEg-_goLPXw7",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!7za -y x \"/content/drive/MyDrive/ds fake and real images/fake.zip\""
      ],
      "metadata": {
        "id": "bPFMIoIePjmy"
      },
      "id": "bPFMIoIePjmy",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e6f9f1f7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "id": "e6f9f1f7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd371bd0",
        "outputId": "0fba20d2-abff-4a37-f1a6-0df1977297bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 140000 files belonging to 2 classes.\n",
            "Shape of the dataset: (None, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "folder_path=\"/content/sample_data/Images\"\n",
        "dataset=tf.keras.utils.image_dataset_from_directory(\n",
        "    folder_path,\n",
        "    image_size=(64,64),\n",
        "    batch_size=16,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                    )\n",
        "dataset_shape = dataset.element_spec[0].shape\n",
        "print(\"Shape of the dataset:\", dataset_shape)"
      ],
      "id": "fd371bd0"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b154fb91",
        "outputId": "fa94e44e-d1e4-4b8b-cede-857b22eb8e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fake', 'real']\n"
          ]
        }
      ],
      "source": [
        "classes=dataset.class_names\n",
        "print(classes)\n"
      ],
      "id": "b154fb91"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "daec3673",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "normalize = Lambda(lambda x: x / 255.0)\n",
        "\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda x, y: (normalize(x), y)\n",
        ")\n",
        "\n"
      ],
      "id": "daec3673"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0b2ab59",
        "outputId": "8dc917bf-5171-478f-b5f3-21492c3e0db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 7000 batches\n",
            "Testing set size: 1750 batches\n"
          ]
        }
      ],
      "source": [
        "# Define the size of the training set (e.g., 80% for training)\n",
        "train_size = 0.8\n",
        "\n",
        "# Calculate the number of batches for training and testing\n",
        "num_batches = len(dataset)\n",
        "num_train_batches = int(train_size * num_batches)\n",
        "num_test_batches = num_batches - num_train_batches\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_dataset = dataset.take(num_train_batches)\n",
        "test_dataset = dataset.skip(num_train_batches)\n",
        "\n",
        "# Print the sizes of the training and testing sets\n",
        "print(f\"Training set size: {num_train_batches} batches\")\n",
        "print(f\"Testing set size: {num_test_batches} batches\")\n"
      ],
      "id": "a0b2ab59"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "da9a50b6"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding\n",
        "train_dataset = train_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=2)))\n",
        "test_dataset = test_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=2)))"
      ],
      "id": "da9a50b6"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "526jON-gIvFF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Define your data augmentation transformations\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    preprocessing.Rescaling(1./255),  # Rescale pixel values to [0, 1]\n",
        "    preprocessing.RandomFlip(\"horizontal\"),  # Randomly flip images horizontally\n",
        "    preprocessing.RandomRotation(0.2),  # Randomly rotate images by up to 20 degrees\n",
        "    preprocessing.RandomZoom(0.2),  # Randomly zoom images by up to 20%\n",
        "    preprocessing.RandomContrast(0.2),  # Randomly adjust image contrast\n",
        "])\n",
        "\n",
        "# Apply data augmentation to your original dataset and create augmented dataset\n",
        "augmented_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Concatenate the augmented data with the original dataset\n",
        "combined_dataset = train_dataset.concatenate(augmented_dataset)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "train_dataset = combined_dataset.shuffle(buffer_size=1000)  # You can adjust the buffer_size as needed"
      ],
      "id": "526jON-gIvFF"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define your CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Conv2D layer with 32 filters, a kernel size of (3, 3), and 'relu' activation\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "\n",
        "# Add a MaxPooling2D layer to down-sample the spatial dimensions\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add another Conv2D layer with 64 filters and a kernel size of (3, 3)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Add another MaxPooling2D layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output before feeding it into Dense layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add Dense layers for classification\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))  # Adjust 'num_classes' based on your task\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2Y1ub3-a1YQv"
      },
      "id": "2Y1ub3-a1YQv",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "vCw4IfXp13ES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd5b932-b40e-4466-8b5f-5fc4f005cc02"
      },
      "id": "vCw4IfXp13ES",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14000/14000 [==============================] - 628s 42ms/step - loss: 0.5833 - accuracy: 0.6352 - val_loss: 0.6914 - val_accuracy: 0.5494\n",
            "Epoch 2/10\n",
            "14000/14000 [==============================] - 545s 38ms/step - loss: 0.5038 - accuracy: 0.6869 - val_loss: 0.6649 - val_accuracy: 0.6797\n",
            "Epoch 3/10\n",
            "14000/14000 [==============================] - 547s 38ms/step - loss: 0.4474 - accuracy: 0.7427 - val_loss: 0.7505 - val_accuracy: 0.5808\n",
            "Epoch 4/10\n",
            "14000/14000 [==============================] - 608s 42ms/step - loss: 0.4019 - accuracy: 0.7809 - val_loss: 0.6091 - val_accuracy: 0.6805\n",
            "Epoch 5/10\n",
            "14000/14000 [==============================] - 606s 42ms/step - loss: 0.3726 - accuracy: 0.7982 - val_loss: 0.7022 - val_accuracy: 0.6027\n",
            "Epoch 6/10\n",
            "14000/14000 [==============================] - 607s 42ms/step - loss: 0.3519 - accuracy: 0.8109 - val_loss: 0.4810 - val_accuracy: 0.7873\n",
            "Epoch 7/10\n",
            "14000/14000 [==============================] - 606s 42ms/step - loss: 0.3386 - accuracy: 0.8194 - val_loss: 0.4921 - val_accuracy: 0.8102\n",
            "Epoch 8/10\n",
            "14000/14000 [==============================] - 569s 39ms/step - loss: 0.3295 - accuracy: 0.8263 - val_loss: 0.4246 - val_accuracy: 0.8283\n",
            "Epoch 9/10\n",
            "14000/14000 [==============================] - 622s 43ms/step - loss: 0.3218 - accuracy: 0.8300 - val_loss: 0.5264 - val_accuracy: 0.8124\n",
            "Epoch 10/10\n",
            "14000/14000 [==============================] - 588s 41ms/step - loss: 0.3161 - accuracy: 0.8355 - val_loss: 0.6974 - val_accuracy: 0.8679\n",
            "Epoch 1/10\n",
            " 8976/14000 [==================>...........] - ETA: 2:29 - loss: 0.1668 - accuracy: 0.9162"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v0tzZvNVWvim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6209c03-7416-4178-df34-f41d4dc9c006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('model.h5') # Saves the entire model to a single artifact"
      ],
      "id": "v0tzZvNVWvim"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}