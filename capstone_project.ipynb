{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danish-Hussain-DSAI/Deep-Fake-on-RESNET50/blob/main/capstone_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyKSsLBYAKGK",
        "outputId": "e4a4ac3d-e327-4d96-e2d9-ca1c460c8d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "pyKSsLBYAKGK"
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/ds fake and real images/real.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/drive/MyDrive/extracted_real')"
      ],
      "metadata": {
        "id": "uJNa0ezit_li"
      },
      "id": "uJNa0ezit_li",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6f9f1f7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "id": "e6f9f1f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd371bd0",
        "outputId": "ed55a960-b5ba-4519-945d-f1a395332516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 15387 files belonging to 2 classes.\n",
            "Shape of the dataset: (None, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "folder_path=\"/content/drive/MyDrive/real\"\n",
        "dataset=tf.keras.utils.image_dataset_from_directory(\n",
        "    folder_path,\n",
        "    image_size=(64,64),\n",
        "    batch_size=32,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                    )\n",
        "dataset_shape = dataset.element_spec[0].shape\n",
        "print(\"Shape of the dataset:\", dataset_shape)"
      ],
      "id": "fd371bd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b154fb91",
        "outputId": "9d32e1d9-63c4-4253-9314-f5c711992e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fake', 'real']\n"
          ]
        }
      ],
      "source": [
        "classes=dataset.class_names\n",
        "print(classes)\n"
      ],
      "id": "b154fb91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daec3673",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "normalize = Lambda(lambda x: x / 255.0)\n",
        "\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda x, y: (normalize(x), y)\n",
        ")\n",
        "\n"
      ],
      "id": "daec3673"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0b2ab59",
        "outputId": "5a6192c3-09a9-4def-b98e-f1855e1b65db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 384 batches\n",
            "Testing set size: 97 batches\n"
          ]
        }
      ],
      "source": [
        "# Define the size of the training set (e.g., 80% for training)\n",
        "train_size = 0.8\n",
        "\n",
        "# Calculate the number of batches for training and testing\n",
        "num_batches = len(dataset)\n",
        "num_train_batches = int(train_size * num_batches)\n",
        "num_test_batches = num_batches - num_train_batches\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_dataset = dataset.take(num_train_batches)\n",
        "test_dataset = dataset.skip(num_train_batches)\n",
        "\n",
        "# Print the sizes of the training and testing sets\n",
        "print(f\"Training set size: {num_train_batches} batches\")\n",
        "print(f\"Testing set size: {num_test_batches} batches\")\n"
      ],
      "id": "a0b2ab59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da9a50b6"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding\n",
        "train_dataset = train_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=2)))\n",
        "test_dataset = test_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=2)))"
      ],
      "id": "da9a50b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "526jON-gIvFF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Define your data augmentation transformations\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    preprocessing.Rescaling(1./255),  # Rescale pixel values to [0, 1]\n",
        "    preprocessing.RandomFlip(\"horizontal\"),  # Randomly flip images horizontally\n",
        "    preprocessing.RandomRotation(0.2),  # Randomly rotate images by up to 20 degrees\n",
        "    preprocessing.RandomZoom(0.2),  # Randomly zoom images by up to 20%\n",
        "    preprocessing.RandomContrast(0.2),  # Randomly adjust image contrast\n",
        "])\n",
        "\n",
        "# Apply data augmentation to your original dataset and create augmented dataset\n",
        "augmented_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Concatenate the augmented data with the original dataset\n",
        "combined_dataset = train_dataset.concatenate(augmented_dataset)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "train_dataset = combined_dataset.shuffle(buffer_size=1000)  # You can adjust the buffer_size as needed"
      ],
      "id": "526jON-gIvFF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "05a670fe",
        "outputId": "fdc70880-c783-4fd0-a6c9-2f362f425a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "768/768 [==============================] - 83s 49ms/step - loss: 0.6782 - accuracy: 0.6201 - val_loss: 0.6245 - val_accuracy: 0.6986\n",
            "Epoch 2/5\n",
            "768/768 [==============================] - 97s 73ms/step - loss: 0.6218 - accuracy: 0.7074 - val_loss: 0.6096 - val_accuracy: 0.6993\n",
            "Epoch 3/5\n",
            "768/768 [==============================] - 99s 73ms/step - loss: 0.6069 - accuracy: 0.7111 - val_loss: 0.6112 - val_accuracy: 0.6986\n",
            "Epoch 4/5\n",
            "768/768 [==============================] - 74s 45ms/step - loss: 0.6061 - accuracy: 0.7111 - val_loss: 0.6090 - val_accuracy: 0.7022\n",
            "Epoch 5/5\n",
            "768/768 [==============================] - 95s 71ms/step - loss: 0.6014 - accuracy: 0.7109 - val_loss: 0.6107 - val_accuracy: 0.7005\n",
            "97/97 [==============================] - 22s 47ms/step - loss: 0.6120 - accuracy: 0.6993\n",
            "Test Accuracy: 69.93%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Define the ResNet model with pre-trained weights\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "# Add custom layers on top of the ResNet base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "\n",
        "x = Dropout(0.5)(x)  # Adding dropout\n",
        "x = BatchNormalization()(x)  # Adding batch normalization\n",
        "predictions = Dense(2, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Optionally, freeze the layers of the ResNet base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model on your training dataset and collect history\n",
        "history = model.fit(train_dataset, epochs=5, batch_size=32, validation_data=test_dataset, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on your testing dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "id": "05a670fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0tzZvNVWvim"
      },
      "outputs": [],
      "source": [
        "model.save('model.h5') # Saves the entire model to a single artifact"
      ],
      "id": "v0tzZvNVWvim"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "TEH8BLzLFEwE",
        "outputId": "c78850e0-374f-452d-d7c6-395890b5242b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c3a31e1980f9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot training and validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_accuracy' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib.rcsetup import validate_color\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(accuracy, label='Training Accuracy')\n",
        "plt.plot(validate_color_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "TEH8BLzLFEwE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b5942c9"
      },
      "outputs": [],
      "source": [],
      "id": "5b5942c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f95c90c0"
      },
      "outputs": [],
      "source": [],
      "id": "f95c90c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64fb71ff"
      },
      "outputs": [],
      "source": [],
      "id": "64fb71ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91a0abce"
      },
      "outputs": [],
      "source": [],
      "id": "91a0abce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7133986"
      },
      "outputs": [],
      "source": [],
      "id": "d7133986"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}